<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>01 论文阅读Vision Transformer with Quadrangle Attention - 萝莉控萝莉的博客</title><meta name=author content><meta name=author-link content><meta name=description content="作者信息 ​​ 一句话总结 摘要 基于窗口的注意力很好用，但是手动设计窗口的参数与输入数据无关，这限制了Transformer对不同物体，大小、形状"><meta name=keywords content="论文阅读,Transformer,注意力机制"><meta itemprop=name content="01 论文阅读Vision Transformer with Quadrangle Attention"><meta itemprop=description content="作者信息 ​​ 一句话总结 摘要 基于窗口的注意力很好用，但是手动设计窗口的参数与输入数据无关，这限制了Transformer对不同物体，大小、形状"><meta itemprop=datePublished content="2024-01-23T13:41:50+08:00"><meta itemprop=dateModified content="2024-01-28T17:33:22+08:00"><meta itemprop=wordCount content="2043"><meta itemprop=image content="/logo.png"><meta itemprop=keywords content="论文阅读,Transformer,注意力机制,"><meta property="og:title" content="01 论文阅读Vision Transformer with Quadrangle Attention"><meta property="og:description" content="作者信息 ​​ 一句话总结 摘要 基于窗口的注意力很好用，但是手动设计窗口的参数与输入数据无关，这限制了Transformer对不同物体，大小、形状"><meta property="og:type" content="article"><meta property="og:url" content="/01-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvision-transformer-with-quadrangle-attention/"><meta property="og:image" content="/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-01-23T13:41:50+08:00"><meta property="article:modified_time" content="2024-01-28T17:33:22+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/logo.png"><meta name=twitter:title content="01 论文阅读Vision Transformer with Quadrangle Attention"><meta name=twitter:description content="作者信息 ​​ 一句话总结 摘要 基于窗口的注意力很好用，但是手动设计窗口的参数与输入数据无关，这限制了Transformer对不同物体，大小、形状"><meta name=application-name content="萝莉控萝莉的博客"><meta name=apple-mobile-web-app-title content="萝莉控萝莉的博客"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=/01-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvision-transformer-with-quadrangle-attention/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"01 论文阅读Vision Transformer with Quadrangle Attention","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"\/01-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvision-transformer-with-quadrangle-attention\/"},"genre":"posts","keywords":"论文阅读, Transformer, 注意力机制","wordcount":2043,"url":"\/01-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvision-transformer-with-quadrangle-attention\/","datePublished":"2024-01-23T13:41:50+08:00","dateModified":"2024-01-28T17:33:22+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"作者"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=萝莉控萝莉的博客><img loading=lazy src=/images/%e5%a4%9c%e7%be%8a.webp data-title=萝莉控萝莉的博客 data-alt=萝莉控萝莉的博客 class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>萝莉控萝莉 lolikonloli</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/>🏠主页</a></li><li class=menu-item><a class=menu-link href=/posts>📚文章</a></li><li class=menu-item><a class=menu-link href=/categories>🗂️分类</a></li><li class=menu-item><a class=menu-link href=/tags>🧩标签</a></li><li class=menu-item><a class=menu-link href=/archives>⏱️时间轴</a></li><li class=menu-item><a class=menu-link href=/about>🙋🏻‍♂️关于</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=萝莉控萝莉的博客><img loading=lazy src=/images/%e5%a4%9c%e7%be%8a.webp data-title=/images/夜羊.webp data-alt=/images/夜羊.webp class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>萝莉控萝莉 lolikonloli</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/>🏠主页</a></li><li class=menu-item><a class=menu-link href=/posts>📚文章</a></li><li class=menu-item><a class=menu-link href=/categories>🗂️分类</a></li><li class=menu-item><a class=menu-link href=/tags>🧩标签</a></li><li class=menu-item><a class=menu-link href=/archives>⏱️时间轴</a></li><li class=menu-item><a class=menu-link href=/about>🙋🏻‍♂️关于</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><nav aria-label=breadcrumb class="breadcrumb-container sticky"><ol class=breadcrumb><li class=breadcrumb-item><a href=/ title=萝莉控萝莉的博客>主页</a></li><li class=breadcrumb-item><a href=/posts/ title=📚文章>📚文章</a></li><li class=breadcrumb-item><a href=/posts/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ title="🚮 论文阅读">🚮 论文阅读</a></li><li class="breadcrumb-item active" aria-current=page>01 论文阅读Vision Transformer with Quadrangle Attention</li></ol></nav><main class="container container-reverse"><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>01 论文阅读Vision Transformer with Quadrangle Attention</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i></span></span>
<span class=post-category>收录于 <a href=/categories/%E7%AC%94%E8%AE%B0/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> 笔记</a></span></div><div class=post-meta-line><span title="发布于 2024-01-23 13:41:50"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2024-01-23>2024-01-23</time></span>&nbsp;<span title="更新于 2024-01-28 17:33:22"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2024-01-28>2024-01-28</time></span>&nbsp;<span title="2043 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 2100 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 5 分钟</span>&nbsp;</div></div><div class=featured-image><img loading=lazy src=https://www.dmoe.cc/random.php?01%20%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bbVision%20Transformer%20with%20Quadrangle%20Attention data-title="https://www.dmoe.cc/random.php?01 论文阅读Vision Transformer with Quadrangle Attention" data-alt="https://www.dmoe.cc/random.php?01 论文阅读Vision Transformer with Quadrangle Attention" style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#作者信息>作者信息</a></li><li><a href=#一句话总结>一句话总结</a></li><li><a href=#摘要>摘要</a></li><li><a href=#动机介绍和相关工作>动机（介绍和相关工作）</a><ul><li><a href=#现有方法遇到的问题>现有方法遇到的问题</a></li><li><a href=#现有的方法是怎么做的为什么这样做不好>现有的方法是怎么做的，为什么这样做不好</a></li><li><a href=#作者是怎么做的>作者是怎么做的。</a></li></ul></li><li><a href=#方法>方法</a><ul><li><a href=#四边形注意力>四边形注意力</a><ul><li><a href=#基本窗口生成>基本窗口生成</a></li><li><a href=#四边生成>四边生成</a></li><li><a href=#正则化>正则化</a></li><li><a href=#模型规模>模型规模</a></li></ul></li></ul></li><li><a href=#实验>实验</a></li><li><a href=#结论>结论</a></li></ul></nav></div></div><div class=content id=content><h1 id=作者信息>作者信息</h1><p>　　​<img loading=lazy src=assets/image-20240123134331-qnvsgmy.png srcset="assets/image-20240123134331-qnvsgmy.png?size=small, assets/image-20240123134331-qnvsgmy.png?size=medium 1.5x, assets/image-20240123134331-qnvsgmy.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>​</p><h1 id=一句话总结>一句话总结</h1><h1 id=摘要>摘要</h1><p>　　基于窗口的注意力很好用，但是手动设计窗口的参数与输入数据无关，这限制了Transformer对不同物体，大小、形状、和方向等属性的感知能力。</p><p>　　因此提出了四边形注意力QA，将基于窗口的注意力扩展到四边形公式中。提出端到端的四边形回归模块，将预测框框变换成预测四边形的Koken。提出了一个框架，在分类，检测，语义分割，姿态识别等任务上表现良好。</p><h1 id=动机介绍和相关工作>动机（介绍和相关工作）</h1><h2 id=现有方法遇到的问题>现有方法遇到的问题</h2><p>　　ViT在视觉任务中很有效，它把输入的图像切成小块并编码，然后把二维图像视为一维序列，使用多头注意力和FFN处理。但是原始的注意力计算Attention Map的复杂度是$o(n^2)$，这导致Transformer在处理高分辨率图像是十分困难的。因此局部窗口的Transformer将图像划分为几个正方形的窗口，在每个窗口内使用注意力，平衡了性能与资源。这种方法对窗口进行了约束，限制了Transformer的长程建模能力和对不同对象的大小，形状和方向的感知，然而这些属性在视觉任务中十分重要。</p><h2 id=现有的方法是怎么做的为什么这样做不好>现有的方法是怎么做的，为什么这样做不好</h2><p>　　以前的研究侧重于设计高级的结构，使得可以更好的进行长程建模，以改进基于窗口的注意力。Swim使用大的滑动窗口（7->32），Focal attention使用coarse granularity tokens捕获长程信息，cross-shaped window attention使用相互垂直的矩形窗口捕获水平和垂直方向的信息，Pale从水平垂直和对角方向建立长程依赖关系。这些方法通过增加注意力的距离提升分类的性能。尽管图像中目标的大小、形状和方向都是任意的，但是这些方法都采用了固定的矩形窗口进行计算。这种与数据无关的窗口设计对于ViT来说可能是次优的。</p><h2 id=作者是怎么做的>作者是怎么做的。</h2><p>　　在这篇研究中，作者提出了一个数据驱动的解决方案，将矩形拓展到四边形，其中的形状大小和方向都是可以自动学习的。它能使Transformer更好的学习到不同的特征，用于表示不同的对象。</p><p>　　提出了新的注意力方法，从数据中学习物体的四边形配置，计算局部注意力。使用默认的窗口对图像进行分区，之后使用端到端的可学习四边形回归模块预测每个窗口变化，将默认窗口转换为目标四边形。</p><p>　　具体来说，作者提出了一种新的注意方法Quadrangle Attention (QA)，用于学习自适应四边形参数，以计算局部注意。它使用默认窗口对输入图像进行分区，并使用端到端可学习的四边形回归模块来预测每个窗口的参数化变换矩阵。变换包括平移、缩放、旋转、剪切和投影，用于将默认窗口转换为目标四边形。为了提高训练的稳定性并允许良好的可解释性，变换矩阵被表述为几个基本变换的组合。与窗口注意不同，在多头自注意(MHSA)层中不同头部之间共享窗口定义不同，所提出的四边形变换对每个头部独立执行。这种设计使注意力层能够对不同的长期依赖关系进行建模，并促进重叠窗口之间的信息交换，而不需要窗口移位 或Token置换。作者提出了QFormer架构，在许多任务上都取得了优异的成绩。</p><p>　　主要贡献有三点：</p><ul><li>提出了一种新的四边形注意力方法</li><li>提出了QFormer的新架构</li><li>实验表明提出了架构在各个代表性的任务上取得了优秀的表现</li></ul><h1 id=方法>方法</h1><h2 id=四边形注意力>四边形注意力</h2><h3 id=基本窗口生成>基本窗口生成</h3><p>　　首先将特征图划分为几个预设大小的窗口</p><h3 id=四边生成>四边生成</h3><p>　　定义如下变换矩阵T：<img loading=lazy src=assets/image-20240128164527-7g6kolu.png srcset="assets/image-20240128164527-7g6kolu.png?size=small, assets/image-20240128164527-7g6kolu.png?size=medium 1.5x, assets/image-20240128164527-7g6kolu.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>，a = [a1, a2;a3, a4]定义了缩放、旋转和剪切的转换，b = [b1;b2]定义了平移，c = [c1, c2]是一个投影向量，它定义了当观察者的视角在深度维度上变化时感知对象如何变化。</p><p>　　直接回归八个参数并不容易，因此将其拆分。首先预测$t \in R^9$,<img loading=lazy src=assets/image-20240128165006-vv471mc.png srcset="assets/image-20240128165006-vv471mc.png?size=small, assets/image-20240128165006-vv471mc.png?size=medium 1.5x, assets/image-20240128165006-vv471mc.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>​</p><p>　　得到t后，根据如下公式得到对应的参数：<img loading=lazy src=assets/image-20240128165144-8g03l6r.png srcset="assets/image-20240128165144-8g03l6r.png?size=small, assets/image-20240128165144-8g03l6r.png?size=medium 1.5x, assets/image-20240128165144-8g03l6r.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>，其中$\beta_1=\frac{W}{w}, \beta_2=\frac{H}{h}$是缩放相关的参数，以帮助使模型适应不同的输入大小</p><p>　　最终的T计算如下：<img loading=lazy src=assets/image-20240128165616-ppkvhta.png srcset="assets/image-20240128165616-ppkvhta.png?size=small, assets/image-20240128165616-ppkvhta.png?size=medium 1.5x, assets/image-20240128165616-ppkvhta.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>​</p><p>　　给定点$(x_1, y_1)$，根据如下计算就能得到变换点的坐标：<br>​<img loading=lazy src=assets/image-20240128165725-y02ez8l.png srcset="assets/image-20240128165725-y02ez8l.png?size=small, assets/image-20240128165725-y02ez8l.png?size=medium 1.5x, assets/image-20240128165725-y02ez8l.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>​<br>​<img loading=lazy src=assets/image-20240128170401-2kfezb5.png srcset="assets/image-20240128170401-2kfezb5.png?size=small, assets/image-20240128170401-2kfezb5.png?size=medium 1.5x, assets/image-20240128170401-2kfezb5.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>​</p><p>　　投影的过程具体如下图所示：<br>​<img loading=lazy src=assets/image-20240128165840-o52h6db.png srcset="assets/image-20240128165840-o52h6db.png?size=small, assets/image-20240128165840-o52h6db.png?size=medium 1.5x, assets/image-20240128165840-o52h6db.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>​</p><p>　　两个不同参数的变换，在绝对坐标系下的对比如下：<br>​<img loading=lazy src=assets/image-20240128170116-ljrtf5o.png srcset="assets/image-20240128170116-ljrtf5o.png?size=small, assets/image-20240128170116-ljrtf5o.png?size=medium 1.5x, assets/image-20240128170116-ljrtf5o.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>​</p><p>　　当$t=0$时，变换后的四边形就是原始的窗口。初始化时，t都会初始化成0.</p><p>　　在同一坐标下使用坐标变换可能会导致歧义。<img loading=lazy src=assets/image-20240128170116-ljrtf5o.png srcset="assets/image-20240128170116-ljrtf5o.png?size=small, assets/image-20240128170116-ljrtf5o.png?size=medium 1.5x, assets/image-20240128170116-ljrtf5o.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>如图所示，a的原点更靠近坐标中心。经过它们都有相同的投影变换矩阵，但是变换前后的差别非常大。因此变换是基于每个窗口的相对坐标来进行的，而不是基于绝对坐标。</p><p>　　​<img loading=lazy src=assets/image-20240128171022-q6w7rm9.png srcset="assets/image-20240128171022-q6w7rm9.png?size=small, assets/image-20240128171022-q6w7rm9.png?size=medium 1.5x, assets/image-20240128171022-q6w7rm9.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>，其中$x^c, y^c$是窗口中心点的坐标，通过变换得到相对坐标，然后使用四边形注意力处理，得到$x_r^q, y_r^q$，然后在使用如下公式变换回去<img loading=lazy src=assets/image-20240128171027-2f4x2a9.png srcset="assets/image-20240128171027-2f4x2a9.png?size=small, assets/image-20240128171027-2f4x2a9.png?size=medium 1.5x, assets/image-20240128171027-2f4x2a9.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>​</p><p>　　获得坐标后，使用网格采样对Key和Value进行采样，得到$K_w, V_w $</p><p>　　四边形的区域可能重叠，但是也可能产生覆盖特征图外区域的四边形，因此对超过特征图的区域填充0，最终使用$K_w, V_w $和原始的$Q_w $计算</p><h3 id=正则化>正则化</h3><p>　　由于有区域可能在特征图之外，这些区域的梯度总是0，这阻碍了四边形回归模块的学习，因此设计了一个正则化项来鼓励投影四边形覆盖有效区域</p><p>　　​<img loading=lazy src=assets/image-20240128172122-ntrdfyw.png srcset="assets/image-20240128172122-ntrdfyw.png?size=small, assets/image-20240128172122-ntrdfyw.png?size=medium 1.5x, assets/image-20240128172122-ntrdfyw.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><br><img loading=lazy src=assets/image-20240128172131-zrvapjd.png srcset="assets/image-20240128172131-zrvapjd.png?size=small, assets/image-20240128172131-zrvapjd.png?size=medium 1.5x, assets/image-20240128172131-zrvapjd.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>，其中λ是超参数</p><h3 id=模型规模>模型规模</h3><p>　　​<img loading=lazy src=assets/image-20240128172308-0efonu9.png srcset="assets/image-20240128172308-0efonu9.png?size=small, assets/image-20240128172308-0efonu9.png?size=medium 1.5x, assets/image-20240128172308-0efonu9.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>​</p><p>　　​<img loading=lazy src=assets/image-20240128172336-hk7g4ti.png srcset="assets/image-20240128172336-hk7g4ti.png?size=small, assets/image-20240128172336-hk7g4ti.png?size=medium 1.5x, assets/image-20240128172336-hk7g4ti.png?size=large 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>​</p><h1 id=实验>实验</h1><h1 id=结论>结论</h1></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-01-28 17:33:22">更新于 2024-01-28&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank>CC BY-NC-SA 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/01-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvision-transformer-with-quadrangle-attention/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ class=post-tag>论文阅读</a><a href=/tags/transformer/ class=post-tag>Transformer</a><a href=/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/ class=post-tag>注意力机制</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav></div></div></article></main></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:red;--bg-progress-dark:red></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/pink/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/lunr/lunr.min.js defer></script><script src=/lib/lunr/lunr.stemmer.support.min.js defer></script><script src=/lib/lunr/lunr.zh.min.js defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script src=/js/live2d_config.js defer></script><script src=https://cdn.jsdelivr.net/npm/live2d-widget@3.1.4/lib/L2Dwidget.min.js defer></script><script src=https://cdn.jsdelivr.net/npm/live2d-widget@3.1.4/lib/L2Dwidget.0.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:10},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{highlightTag:"em",lunrIndexURL:"/index.json",lunrLanguageCode:"zh",lunrSegmentitURL:"/lib/lunr/lunr.segmentit.js",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"lunr"}}</script><script src=/js/theme.min.js defer></script></body></html>