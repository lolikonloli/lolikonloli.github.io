<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃 - 萝莉控萝莉的博客</title><meta name=author content>
<meta name=author-link content><meta name=description content="相关信息 作者 <img src=&#34;static/posts/论文阅读/论文阅读_Analyzing_Feed-Forward_Blocks_"><meta name=keywords content="论文笔记,注意力机制"><meta itemprop=name content="论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃"><meta itemprop=description content="相关信息 作者 <img src=&#34;static/posts/论文阅读/论文阅读_Analyzing_Feed-Forward_Blocks_"><meta itemprop=datePublished content="2024-02-04T13:41:50+08:00"><meta itemprop=dateModified content="2024-02-04T13:41:50+08:00"><meta itemprop=wordCount content="554"><meta itemprop=image content="/logo.png"><meta itemprop=keywords content="论文笔记,注意力机制,"><meta property="og:title" content="论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃"><meta property="og:description" content="相关信息 作者 <img src=&#34;static/posts/论文阅读/论文阅读_Analyzing_Feed-Forward_Blocks_"><meta property="og:type" content="article"><meta property="og:url" content="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83/"><meta property="og:image" content="/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-04T13:41:50+08:00"><meta property="article:modified_time" content="2024-02-04T13:41:50+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/logo.png"><meta name=twitter:title content="论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃"><meta name=twitter:description content="相关信息 作者 <img src=&#34;static/posts/论文阅读/论文阅读_Analyzing_Feed-Forward_Blocks_"><meta name=application-name content="萝莉控萝莉的博客"><meta name=apple-mobile-web-app-title content="萝莉控萝莉的博客"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83/><link rel=prev href=/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvision-transformer-with-quadrangle-attention/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"\/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83\/"},"genre":"posts","keywords":"论文笔记, 注意力机制","wordcount":554,"url":"\/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83\/","datePublished":"2024-02-04T13:41:50+08:00","dateModified":"2024-02-04T13:41:50+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"作者"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=wide><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=萝莉控萝莉的博客><img loading=lazy src=/images/%e5%a4%9c%e7%be%8a.webp alt=萝莉控萝莉的博客 data-title=萝莉控萝莉的博客 class=logo style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>萝莉控萝莉 lolikonloli</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/archives>⏱️时间轴</a></li><li class=menu-item><a class=menu-link href=/>🏠主页</a></li><li class=menu-item><a class=menu-link href=/categories>🗂️分类</a></li><li class=menu-item><a class=menu-link href=/tags>🧩标签</a></li><li class=menu-item><a class=menu-link href=/about>🙋🏻‍♂️关于</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=萝莉控萝莉的博客><img loading=lazy src=/images/%e5%a4%9c%e7%be%8a.webp alt=/images/夜羊.webp data-title=/images/夜羊.webp class=logo style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>萝莉控萝莉 lolikonloli</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/archives>⏱️时间轴</a></li><li class=menu-item><a class=menu-link href=/>🏠主页</a></li><li class=menu-item><a class=menu-link href=/categories>🗂️分类</a></li><li class=menu-item><a class=menu-link href=/tags>🧩标签</a></li><li class=menu-item><a class=menu-link href=/about>🙋🏻‍♂️关于</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><nav aria-label=breadcrumb class="breadcrumb-container sticky"><ol class=breadcrumb><li class=breadcrumb-item><a href=/ title=萝莉控萝莉的博客>主页</a></li><li class=breadcrumb-item><a href=/posts/ title=Posts>文章</a></li><li class="breadcrumb-item active" aria-current=page>论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃</li></ol></nav><main class="container container-reverse"><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/%E8%AE%BA%E6%96%87/ class=post-category title="分类 - 论文"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> 论文</a></span></div><div class=post-meta-line><span title="发布于 <no value>"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2024-02-04>2024-02-04</time></span>&nbsp;<span title><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 600 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 2 分钟</span>&nbsp;</div></div><div class=featured-image><img loading=lazy src=https://www.dmoe.cc/random.php?%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb%20Analyzing%20Feed-Forward%20Blocks%20in%20Transformers%20Through%20Lens%20of%20Attention%20Map%20%e6%94%be%e5%bc%83 alt="https://www.dmoe.cc/random.php?论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃" data-title="https://www.dmoe.cc/random.php?论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃" style="background:url(/images/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title;for(const e of["style","data-title","onerror","onload"])this.removeAttribute(e)'></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#相关信息>相关信息</a><ul><li><a href=#作者>作者</a></li><li><a href=#相关链接>相关链接</a></li></ul></li><li><a href=#一句话总结>一句话总结</a></li><li><a href=#摘要>摘要</a></li><li><a href=#动机介绍和相关工作>动机（介绍和相关工作）</a><ul><li><a href=#跳过背景介绍部分>跳过背景介绍部分</a></li><li><a href=#作者是怎么做的>作者是怎么做的</a></li></ul></li><li><a href=#方法>方法</a></li><li><a href=#实验>实验</a></li><li><a href=#结论>结论</a></li></ul></nav></div></div><div class=content id=content><h1 id=相关信息 class=heading-element><a href=#%e7%9b%b8%e5%85%b3%e4%bf%a1%e6%81%af class=heading-mark></a>相关信息</h1><h2 id=作者 class=heading-element><a href=#%e4%bd%9c%e8%80%85 class=heading-mark></a>作者</h2><p>　　<code>&lt;img src="static/posts/论文阅读/论文阅读_Analyzing_Feed-Forward_Blocks_in_Transformers_Through_Lens_of_Attention_Map_放弃/image-20240204132614-5m4ty3b.webp" width="auto" height="auto"></code></p><h2 id=相关链接 class=heading-element><a href=#%e7%9b%b8%e5%85%b3%e9%93%be%e6%8e%a5 class=heading-mark></a>相关链接</h2><h1 id=一句话总结 class=heading-element><a href=#%e4%b8%80%e5%8f%a5%e8%af%9d%e6%80%bb%e7%bb%93 class=heading-mark></a>一句话总结</h1><p>　　这是一篇分析语言模型的 Transforemr 内部结构，特别是 FFN 的影响的文章。提出了一种新方法可视化特征图，只需要前向传播。</p><p>　　看不下去一点</p><h1 id=摘要 class=heading-element><a href=#%e6%91%98%e8%a6%81 class=heading-mark></a>摘要</h1><p>　　 Transformer 很多人在用，因此解释其内部就很重要。FFN 的分析都没什么人做。作者通过可视化分析了 FFN 对上下文的影响。通过对 masked-language models 和 causal-language models 的分析，发现了 FFN 强调了特定类型的语言组合。此外，FFN 和周围的模块通常会抵消彼此的影响，这说明 Transforemr 层中存在潜在的冗余</p><h1 id=动机介绍和相关工作 class=heading-element><a href=#%e5%8a%a8%e6%9c%ba%e4%bb%8b%e7%bb%8d%e5%92%8c%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c class=heading-mark></a>动机（介绍和相关工作）</h1><h2 id=跳过背景介绍部分 class=heading-element><a href=#%e8%b7%b3%e8%bf%87%e8%83%8c%e6%99%af%e4%bb%8b%e7%bb%8d%e9%83%a8%e5%88%86 class=heading-mark></a>跳过背景介绍部分</h2><h2 id=作者是怎么做的 class=heading-element><a href=#%e4%bd%9c%e8%80%85%e6%98%af%e6%80%8e%e4%b9%88%e5%81%9a%e7%9a%84 class=heading-mark></a>作者是怎么做的</h2><p>　　作者提出了一种基于范数的方法用于分析 FFN 对特征图的影响。该方法有几个优点</p><ul><li>不需要反向传播，只需要前向传播</li><li>对语言模型的研究发现了 FFN 的上下文效应（contextualization effects）。具体来说发现了特定层中的 FFN 和归一化往往在很大程度上控制着上下文；观察到了典型的 FFN 效应，独立于语言模型；发现 FFN 的影响会被周围的残差层和归一化层削弱，这说明 Transformer 内部存在冗余</li></ul><h1 id=方法 class=heading-element><a href=#%e6%96%b9%e6%b3%95 class=heading-mark></a>方法</h1><h1 id=实验 class=heading-element><a href=#%e5%ae%9e%e9%aa%8c class=heading-mark></a>实验</h1><h1 id=结论 class=heading-element><a href=#%e7%bb%93%e8%ae%ba class=heading-mark></a>结论</h1></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2024-02-04 13:41:50">更新于 2024-02-04&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank>CC BY-NC-SA 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ class=post-tag title="标签 - 论文笔记">论文笔记</a><a href=/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/ class=post-tag title="标签 - 注意力机制">注意力机制</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvision-transformer-with-quadrangle-attention/ class=post-nav-item rel=prev title=论文阅读Vision_Transformer_with_Quadrangle_Attention><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>论文阅读Vision_Transformer_with_Quadrangle_Attention</a></div></div></article><aside class=toc id=toc-auto aria-label><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class="toc-content always-active" id=toc-content-auto></div></aside></main></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:red;--bg-progress-dark:red></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=/lib/katex/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.css></noscript><link rel=stylesheet href=/lib/cookieconsent/cookieconsent.min.css><link rel=stylesheet href=/lib/pace/themes/pink/pace-theme-minimal.css><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script src=/lib/pace/pace.min.js async defer></script><script src=/js/live2d_config.js defer></script><script src=https://cdn.jsdelivr.net/npm/live2d-widget@3.1.4/lib/L2Dwidget.min.js defer></script><script src=https://cdn.jsdelivr.net/npm/live2d-widget@3.1.4/lib/L2Dwidget.0.min.js defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:10},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},enablePWA:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50}}</script><script src=/js/theme.min.js defer></script></body></html>