<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>所有文章 - 萝莉控萝莉的博客</title><link>/posts/</link><description>所有文章 | 萝莉控萝莉的博客</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Sun, 04 Feb 2024 13:41:50 +0800</lastBuildDate><atom:link href="/posts/" rel="self" type="application/rss+xml"/><item><title>论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃</title><link>/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83/</link><pubDate>Sun, 04 Feb 2024 13:41:50 +0800</pubDate><author>作者</author><guid>/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83/</guid><description><![CDATA[<div class="featured-image"><img loading="eager" src="https://www.dmoe.cc/random.php?%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb%20Analyzing%20Feed-Forward%20Blocks%20in%20Transformers%20Through%20Lens%20of%20Attention%20Map%20%e6%94%be%e5%bc%83" alt="论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃" title="论文阅读 Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃" referrerpolicy="no-referrer"/></div>相关信息 作者 &lt;img src=&quot;static/posts/论文阅读/论文阅读_Analyzing_Feed-Forward_Blocks_]]></description></item><item><title>论文阅读Vision_Transformer_with_Quadrangle_Attention</title><link>/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvision-transformer-with-quadrangle-attention/</link><pubDate>Tue, 23 Jan 2024 13:41:50 +0800</pubDate><author>作者</author><guid>/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBvision-transformer-with-quadrangle-attention/</guid><description>&lt;div class="featured-image">&lt;img loading="eager" src="https://www.dmoe.cc/random.php?01%20%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bbVision%20Transformer%20with%20Quadrangle%20Attention" alt="论文阅读Vision_Transformer_with_Quadrangle_Attention" title="论文阅读Vision_Transformer_with_Quadrangle_Attention" referrerpolicy="no-referrer"/>&lt;/div>作者信息 一句话总结 摘要 基于窗口的注意力很好用，但是手动设计窗口的参数与输入数据无关，这限制了Transformer对不同物体，大小、形状、和</description></item><item><title>闭包与工厂函数</title><link>/%E9%97%AD%E5%8C%85%E4%B8%8E%E5%B7%A5%E5%8E%82%E5%87%BD%E6%95%B0/</link><pubDate>Mon, 01 Jan 2024 13:41:50 +0800</pubDate><author>作者</author><guid>/%E9%97%AD%E5%8C%85%E4%B8%8E%E5%B7%A5%E5%8E%82%E5%87%BD%E6%95%B0/</guid><description>&lt;div class="featured-image">&lt;img loading="eager" src="https://www.dmoe.cc/random.php?%e9%97%ad%e5%8c%85" alt="闭包与工厂函数" title="闭包与工厂函数" referrerpolicy="no-referrer"/>&lt;/div>闭包 闭包是一个函数，它记住了它被创建时的环境。更具体地说，闭包可以访问在其外部作用域定义的非全局变量，即使在其外部作用域的生命周期已经结束时</description></item><item><title>构建自己的Hugo博客</title><link>/%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84hugo%E5%8D%9A%E5%AE%A2/</link><pubDate>Fri, 24 Nov 2023 21:40:32 +0800</pubDate><author>作者</author><guid>/%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84hugo%E5%8D%9A%E5%AE%A2/</guid><description>&lt;div class="featured-image">&lt;img loading="eager" src="https://api.likepoems.com/img/pc/?%e6%9e%84%e5%bb%ba%e8%87%aa%e5%b7%b1%e7%9a%84Hugo%e5%8d%9a%e5%ae%a2" alt="构建自己的Hugo博客" title="构建自己的Hugo博客" referrerpolicy="no-referrer"/>&lt;/div>安装软件 安装hugo 打开hugo的github链接: https://github.com/gohugoio/hugo 点击右侧的release，下滑页面后选择hugo_extened_xx版本号xx_wi</description></item></channel></rss>