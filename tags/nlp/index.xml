<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>NLP - 标签 - 萝莉控萝莉的博客</title><link>/tags/nlp/</link><description>NLP - 标签 | 萝莉控萝莉的博客</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Fri, 09 Feb 2024 08:09:59 +0800</lastBuildDate><atom:link href="/tags/nlp/" rel="self" type="application/rss+xml"/><item><title>Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃</title><link>/analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83/</link><pubDate>Fri, 09 Feb 2024 08:09:59 +0800</pubDate><author>作者</author><guid>/analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83/</guid><description>&lt;div class="featured-image">&lt;img loading="eager" src="randomImage" alt="Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃" title="Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃" referrerpolicy="no-referrer"/>&lt;/div>相关信息 作者 相关链接 一句话总结 这是一篇分析语言模型的 Transforemr 内部结构，特别是 FFN 的影响的文章。提出了一种新方法可视化特征图，只需要前向传播。 看不下去</description></item></channel></rss>