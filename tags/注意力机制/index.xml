<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>注意力机制 - 标签 - 萝莉控萝莉的博客</title><link>/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link><description>注意力机制 - 标签 | 萝莉控萝莉的博客</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Fri, 09 Feb 2024 08:09:59 +0800</lastBuildDate><atom:link href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" rel="self" type="application/rss+xml"/><item><title>Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃</title><link>/analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83/</link><pubDate>Fri, 09 Feb 2024 08:09:59 +0800</pubDate><author>作者</author><guid>/analyzing-feed-forward-blocks-in-transformers-through-lens-of-attention-map-%E6%94%BE%E5%BC%83/</guid><description>&lt;div class="featured-image">&lt;img loading="eager" src="randomImage" alt="Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃" title="Analyzing Feed-Forward Blocks in Transformers Through Lens of Attention Map 放弃" referrerpolicy="no-referrer"/>&lt;/div>相关信息 作者 相关链接 一句话总结 这是一篇分析语言模型的 Transforemr 内部结构，特别是 FFN 的影响的文章。提出了一种新方法可视化特征图，只需要前向传播。 看不下去</description></item><item><title>Vision Transformer with Quadrangle Attention</title><link>/vision-transformer-with-quadrangle-attention/</link><pubDate>Fri, 09 Feb 2024 08:09:59 +0800</pubDate><author>作者</author><guid>/vision-transformer-with-quadrangle-attention/</guid><description>&lt;div class="featured-image">&lt;img loading="eager" src="randomImage" alt="Vision Transformer with Quadrangle Attention" title="Vision Transformer with Quadrangle Attention" referrerpolicy="no-referrer"/>&lt;/div>作者信息 一句话总结 摘要 基于窗口的注意力很好用，但是手动设计窗口的参数与输入数据无关，这限制了 Transformer 对不同物体，大小、形状、和方向等属性的感知能力</description></item></channel></rss>